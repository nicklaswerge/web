---
---

@string{neurips = {Neural Information Processing Systems}}
@string{iclr = {International Conference on Learning Representations}}
@string{icml = {International Conference on Machine Learning}}
@string{arxiv = {arXiv Preprint}}
@string{pami = {IEEE Transactions on Pattern Analysis and Macahine Intelligence}}

@inproceedings{yildiz2022learning,
  abbr={NeurIPS},
  title={Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs},
  author={Yildiz, C. and Kandemir, M. and Rakitsch, B.},
  booktitle=neurips,
  year={2022},
  url={https://arxiv.org/abs/2205.11894},
  html={https://arxiv.org/abs/2205.11894},
  selected={true},
  abstract={We study for the first time uncertainty-aware modeling of continuous-time dynamics of interacting objects. We introduce a new model that decomposes independent dynamics of single objects accurately from their interactions. By employing latent Gaussian process ordinary differential equations, our model infers both independent dynamics and their interactions with reliable uncertainty estimates. In our formulation, each object is represented as a graph node and interactions are modeled by accumulating the messages coming from neighboring objects. We show that efficient inference of such a complex network of variables is possible with modern variational sparse Gaussian process inference techniques. We empirically demonstrate that our model improves the reliability of long-term predictions over neural network based alternatives and it successfully handles missing dynamic or static information. Furthermore, we observe that only our model can successfully encapsulate independent dynamics and interaction information in distinct functions and show the benefit from this disentanglement in extrapolation scenarios..},
  bibtex_show={true}
}

@inproceedings{kandemir2022evidential,
  abbr={ICLR},
  title={Evidential Turing Processes },
  author={Kandemir, M. and Akgül, A. and Haussmann, M. and Unal, G.},
  booktitle=iclr,
  year={2022},
  url={https://openreview.net/forum?id=84NMXTHYe-},
  html={https://openreview.net/forum?id=84NMXTHYe-},
  selected={true},
  abstract={A probabilistic classifier with reliable predictive uncertainties i) fits successfully to the target domain data, ii) provides calibrated class probabilities in difficult regions of the target domain (e.g. class overlap), and iii) accurately identifies queries coming out of the target domain and reject them. We introduce an original combination of Evidential Deep Learning, Neural Processes, and Neural Turing Machines capable of providing all three essential properties mentioned above for total uncertainty quantification. We observe our method on three image classification benchmarks to consistently improve the in-domain uncertainty quantification, out-of-domain detection, and robustness against input perturbations with one single model. Our unified solution delivers an implementation-friendly and computationally efficient recipe for safety clearance and provides intellectual economy to an investigation of algorithmic roots of epistemic awareness in deep neural nets.},
  bibtex_show={true}
}

@inproceedings{akgul2022cddp,
  abbr={arXiv},
  title={Continual Learning of Multi-modal Dynamics with External Memory},
  author = {Akgül, A. and Unal, G. and Kandemir, M.},
  booktitle=arxiv,
  year={2022},
  url={https://arxiv.org/abs/2203.00936},
  html={https://arxiv.org/abs/2203.00936},
  selected={false},
  abstract={We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. We devise a novel continual learning method that maintains a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transition kernel as control input. We observe the continual learning performance of our method to compare favorably to the mainstream parameter transfer approach.},
  bibtex_show={true}
}


@article{look2022adeterministic,
  abbr={T-PAMI},
  title={A Deterministic Approximation to Neural SDEs},
  author={Look, A. and Kandemir, M. and Rakitsch, B. and Peters, J.},
  journal=pami,
  year={2022},
  url={https://arxiv.org/abs/2006.08973},
  html={https://arxiv.org/abs/2006.08973},
  publisher={IEEE},
  selected={true},
  bibtex_show={true},
  abstract={Neural Stochastic Differential Equations (NSDEs) model the drift and diffusion functions of a stochastic process as neural networks. While NSDEs are known to make accurate predictions, their uncertainty quantification properties haven been remained unexplored so far. We report the empirical finding that obtaining well-calibrated uncertainty estimations from NSDEs is computationally prohibitive. As a remedy, we develop a computationally affordable deterministic scheme which accurately approximates the transition kernel, when dynamics is governed by a NSDE. Our method introduces a bidimensional moment matching algorithm: vertical along the neural net layers and horizontal along the time direction, which benefits from an original combination of effective approximations. Our deterministic approximation of the transition kernel is applicable  to both training and prediction. We observe in multiple experiments that the uncertainty calibration quality of our method can be matched by Monte Carlo sampling only after introducing high computation cost. Thanks to the numerical stability of deterministic training, our method also provides improvement in prediction accuracy.}
}
